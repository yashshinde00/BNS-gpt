from llama_stack import Llama

# Initialize the Llama model
llama_model_path = "path/to/your/llama3.2-model-directory"  # Replace with the actual path where the Llama 3.2 model is stored
model = Llama(model_path=llama_model_path)

def generate_response(prompt):
    """
    Generate a response from the Llama model.
    
    Args:
        prompt (str): The user input or query.

    Returns:
        str: The response generated by the Llama model.
    """
    try:
        response = model.chat(
            prompt=prompt,
            max_tokens=256,  # Set the max number of tokens for the response
            temperature=0.7,  # Controls randomness; lower is more deterministic
            top_p=0.9        # Controls diversity of responses; higher explores more options
        )
        return response['text']
    except Exception as e:
        return f"Error generating response: {str(e)}"

# Example usage
if __name__ == "__main__":
    print("Llama Inference Test")
    user_prompt = input("Enter your prompt: ")
    response = generate_response(user_prompt)
    print(f"\nLlama's Response:\n{response}")
