from llama_inference import generate_response

def handle_query(user_query):
    """
    Handles a user query and generates a response using the Llama model.

    Args:
        user_query (str): The input query from the user.

    Returns:
        str: The response generated by the Llama model.
    """
    try:
        print("Processing the query...")
        response = generate_response(user_query)
        return response
    except Exception as e:
        return f"Error handling query: {str(e)}"

# Example usage
if __name__ == "__main__":
    print("Query Handler Test")
    user_query = input("Enter your query: ")
    response = handle_query(user_query)
    print(f"\nModel's Response:\n{response}")
